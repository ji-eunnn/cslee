{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "# 어간 추출(Stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer   : ['fli', 'fli', 'fli', 'flew', 'flown']\n",
      "Lancaster Stemmer: ['fly', 'fli', 'fly', 'flew', 'flown']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "\n",
    "st1 = PorterStemmer()\n",
    "st2 =  LancasterStemmer()\n",
    "\n",
    "words = [\"fly\", \"flies\", \"flying\", \"flew\", \"flown\"]\n",
    "\n",
    "print(\"Porter Stemmer   :\", [st1.stem(w) for w in words])\n",
    "print(\"Lancaster Stemmer:\", [st2.stem(w) for w in words])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "# 원형 복원(Lemmatizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fly', 'fly', 'fly', 'fly', 'fly']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lm = WordNetLemmatizer()\n",
    "\n",
    "# 품사(pos)를 동사(v)로 원형 복원\n",
    "[lm.lemmatize(w, pos=\"v\") for w in words]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "# 형태소분석(Part of Speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Emma', 'refused', 'to', 'permit', 'us', 'to', 'obtain', 'the', 'refuse', 'permit']\n",
      "\n",
      "[('Emma', 'NNP'), ('refused', 'VBD'), ('to', 'TO'), ('permit', 'VB'), ('us', 'PRP'), ('to', 'TO'), ('obtain', 'VB'), ('the', 'DT'), ('refuse', 'NN'), ('permit', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "sentence = \"Emma refused to permit us to obtain the refuse permit\"\n",
    "\n",
    "# 토큰 분리\n",
    "tokens = word_tokenize(sentence)\n",
    "print(tokens)\n",
    "print()\n",
    "\n",
    "# 형태소분석\n",
    "tagged_list = pos_tag(tokens)\n",
    "print(tagged_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('tagsets')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset(\"NNP\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset(\"VBD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset(\"DT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['refuse', 'permit']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 품사가 NN인 것만 추출\n",
    "nouns_list = [t[0] for t in tagged_list if t[1] == \"NN\"]\n",
    "nouns_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Emma',\n",
       " 'refused',\n",
       " 'to',\n",
       " 'permit',\n",
       " 'us',\n",
       " 'to',\n",
       " 'obtain',\n",
       " 'the',\n",
       " 'refuse',\n",
       " 'permit']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import untag\n",
    "\n",
    "# 품사 태그 제거\n",
    "untag(tagged_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Emma/NNP',\n",
       " 'refused/VBD',\n",
       " 'to/TO',\n",
       " 'permit/VB',\n",
       " 'us/PRP',\n",
       " 'to/TO',\n",
       " 'obtain/VB',\n",
       " 'the/DT',\n",
       " 'refuse/NN',\n",
       " 'permit/NN']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어와 품사를 합쳐서 하나의 토큰으로 변환\n",
    "word_tag = [\"/\".join(p) for p in tagged_list]\n",
    "word_tag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "# 구문분석(Syntax Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n"
     ]
    }
   ],
   "source": [
    "# 품사 정보가 있는 문장 설정\n",
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"), (\"dog\", \"NN\"), \n",
    "            (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
    "\n",
    "# 문법 정의\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "# 정규표현식 파서\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(sentence)\n",
    "\n",
    "# 결과 출력\n",
    "print(result)\n",
    "\n",
    "# 결과를 트리로 표시\n",
    "result.draw()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "# 한글 형태소분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이썬에서는 보통 KoNLPy(코엔엘파이)를 사용하여 한글 형태소분석을 합니다. Hannanum, Kkma, Komoran, Mecab, Okt(Twitter) 등 여러가지 라이브러리를 동일한 인터페이스로 실행이 가능합니다. \n",
    "\n",
    "KoNLPy는 자바로 구현이 되어 있습니다. 윈도우 아나콘다는 아래와 같은 작업을 거쳐야 합니다. 설치 후 주피터 노트북을 다시 시작해야 자바가 실행됩니다.\n",
    "\n",
    "< 1. 자바 설치 ><br>\n",
    "커맨드창에서 'java -version' 명령어를 실행하여 버전이 1.7 이상이어야 합니다.<br>\n",
    "\n",
    "< 2. JPype1 설치 ><br>\n",
    "파이썬에서 자바를 호출할 수 있는 라이브러리입니다.\n",
    "\n",
    "< 3. KoNLPy 설치 ><br>\n",
    "위의 두 가지가 미리 완료되어 있어야만 가능합니다.\n",
    "\n",
    "아래 링크에서 좀 더 자세한 방법을 확인하시기 바랍니다.<br>\n",
    "https://ericnjennifer.github.io/python_visualization/2018/01/21/PythonVisualization_Chapt1.html<br>\n",
    "https://zetawiki.com/wiki/%EC%9C%88%EB%8F%84%EC%9A%B0_JAVA_HOME_%ED%99%98%EA%B2%BD%EB%B3%80%EC%88%98_%EC%84%A4%EC%A0%95\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\jpype\\_core.py:210: UserWarning: \n",
      "-------------------------------------------------------------------------------\n",
      "Deprecated: convertStrings was not specified when starting the JVM. The default\n",
      "behavior in JPype will be False starting in JPype 0.8. The recommended setting\n",
      "for new code is convertStrings=False.  The legacy value of True was assumed for\n",
      "this session. If you are a user of an application that reported this warning,\n",
      "please file a ticket with the developer.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "  \"\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['자연어', '자연어처리', '처리', '컴퓨터', '인간', '언어', '인공', '인공지능', '지능']\n",
      "['자연어', '처리', '는', '컴퓨터', '가', '인간', '의', '언어', '를', '처리', '하', '도록', '하', '는', '인공지능', '이', 'ㅂ니다', '.']\n",
      "[('자연어', 'NNG'), ('처리', 'NNG'), ('는', 'JX'), ('컴퓨터', 'NNG'), ('가', 'JKS'), ('인간', 'NNG'), ('의', 'JKG'), ('언어', 'NNG'), ('를', 'JKO'), ('처리', 'NNG'), ('하', 'XSV'), ('도록', 'ECD'), ('하', 'VV'), ('는', 'ETD'), ('인공지능', 'NNG'), ('이', 'VCP'), ('ㅂ니다', 'EFN'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "sentence = '자연어처리는 컴퓨터가 인간의 언어를 처리하도록 하는 인공지능입니다.'\n",
    "\n",
    "# 꼬꼬마 형태소분석기 사용\n",
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()\n",
    "\n",
    "print(kkma.nouns(sentence))    # 명사\n",
    "print(kkma.morphs(sentence))   # 형태소\n",
    "print(kkma.pos(sentence))      # 형태소와 품사\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['자연어', '처리', '컴퓨터', '인간', '언어', '처리', '인공지능']\n",
      "['자연어', '처리', '는', '컴퓨터', '가', '인간', '의', '언어', '를', '처리', '하', '도록', '하', '는', '인공지능', '이', 'ㅂ니다', '.']\n",
      "[('자연어', 'NNP'), ('처리', 'NNP'), ('는', 'JX'), ('컴퓨터', 'NNG'), ('가', 'JKS'), ('인간', 'NNG'), ('의', 'JKG'), ('언어', 'NNG'), ('를', 'JKO'), ('처리', 'NNG'), ('하', 'XSV'), ('도록', 'EC'), ('하', 'VV'), ('는', 'ETM'), ('인공지능', 'NNP'), ('이', 'VCP'), ('ㅂ니다', 'EF'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "# 코모란 형태소분석기 사용\n",
    "from konlpy.tag import Komoran\n",
    "komoran = Komoran()\n",
    "\n",
    "print(komoran.nouns(sentence))    # 명사\n",
    "print(komoran.morphs(sentence))   # 형태소\n",
    "print(komoran.pos(sentence))      # 형태소와 품사\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "# KoNLPy 말뭉치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대한민국헌법\n",
      "\n",
      "유구한 역사와 전통에 빛나는 우리 대한국민은 3·1운동으로 건립된 대한민국임시정부의 법통과 불의에 항거한 4·19민주이념을 계승하고, 조국의 민주개혁과 평화적 통일의 사명에 입각하여 정의·인도와 동포애로써 민족의 단결을 공고히 하고, 모든 사회적 폐습과 불의를 타파하며, 자율과 조화를 바탕으로 자유민주적 기본질서를 더욱 확고히 하여 정치·경제·사회·문화의 모든 영역에 있어서 각인의 기회를 균등히 하고, 능력을 최고도로 발휘하게 하며, 자유와 권리에 따르는 책임과 의무를 완수하게 하여, 안으로는 국민생활의 균등한 향상을 기하고 밖으로는 항구적인 세계평화와 인류공영에 이바지함으로써 우리들과 우리들의 자손의 안전과 자유와 행복을 영원히 확보할 것을 다짐하면서 1948년 7월 12일에 제정되고 8차에 걸쳐 개정된 헌법을 이제 국회의 의결을 거쳐 국민투표에 의하여 개정한다.\n",
      "\n",
      "       제1장 총강\n",
      "  제1조 ① 대한민국은 민주공화국이다.\n",
      "②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.\n",
      "  제2조 ① 대한민국의 국민이 되는 요건은 법률로 정한다.\n",
      "②국가는 법률이 정하는 바에 의하여 재외국민을 보호할 의무를 진다.\n",
      "  제3조 대한민국의 영토는 한반도와 그 부속도서로 한다.\n",
      "  제4조 대한민국은 통일을 지향하며, 자유민주적 기본질서에 입각한 평화적 통일 정책을 수립하고 이를 추진한다.\n",
      "  제5조 ① 대한민국은 국제평화의 유지에 노력하고 침략적 전쟁을 부인한다.\n",
      "②국군은 국가의 안전보장과 국토방위의 신성한 의무를 수행함을 사명으로 하며, 그 정치적 중립성은 준수된다.\n",
      "  제6조 ① 헌법에 의하여 체결·공포된 조약과 일반적으로 승인된 국제법규는 국내법과 같은 효력을 가진다.\n",
      "②외국인은 국제법과 조약이 정하는 바에 의하여 그 지위가 보장된다.\n",
      "  제7조 ① 공무원은 국민전체에 대한 봉사자이며, 국민에 대하여 책임을 진다.\n",
      "②공무원의 신분과 정치적 중립성은 법률이 정하는 바에 의하여 보장된다.\n",
      "  제8조 ① 정당의 설립은 자유이며, 복수정당제\n"
     ]
    }
   ],
   "source": [
    "from konlpy.corpus import kolaw\n",
    "\n",
    "# 한국 법률 말뭉치\n",
    "text = kolaw.open('constitution.txt').read()\n",
    "print(text[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지방공무원법 일부개정법률안\n",
      "\n",
      "(정의화의원 대표발의 )\n",
      "\n",
      " 의 안\n",
      " 번 호\n",
      "\n",
      "9890\n",
      "\n",
      "발의연월일 : 2010.  11.  12.  \n",
      "\n",
      "발  의  자 : 정의화․이명수․김을동 \n",
      "\n",
      "이사철․여상규․안규백\n",
      "\n",
      "황영철․박영아․김정훈\n",
      "\n",
      "김학송 의원(10인)\n",
      "\n",
      "제안이유 및 주요내용\n",
      "\n",
      "  초등학교 저학년의 경우에도 부모의 따뜻한 사랑과 보살핌이 필요\n",
      "\n",
      "한 나이이나, 현재 공무원이 자녀를 양육하기 위하여 육아휴직을 할 \n",
      "\n",
      "수 있는 자녀의 나이는 만 6세 이하로 되어 있어 초등학교 저학년인 \n",
      "\n",
      "자녀를 돌보기 위해서는 해당 부모님은 일자리를 그만 두어야\n"
     ]
    }
   ],
   "source": [
    "from konlpy.corpus import kobill\n",
    "\n",
    "# 대한민국 국회 의안 말뭉치\n",
    "text = kobill.open('1809890.txt').read()\n",
    "print(text[:300])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "# 한글 구문분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP 만/Noun 6/Number 세/Noun 이하/Noun)\n",
      "  의/Josa\n",
      "  (NP 초등학교/Noun 취학/Noun 전/Noun 자녀/Noun)\n",
      "  를/Josa\n",
      "  (NP 양육/Noun)\n",
      "  (VP 하기/Verb)\n",
      "  (NP 위/Noun)\n",
      "  (VP 해서는/Verb))\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "# 형태소분석\n",
    "sentence = \"만 6세 이하의 초등학교 취학 전 자녀를 양육하기 위해서는\"\n",
    "words = Okt().pos(sentence)\n",
    "\n",
    "# 문법 정의\n",
    "grammar = \"\"\"\n",
    "NP: {<N.*>*<Suffix>?}   # Noun phrase\n",
    "VP: {<V.*>*}            # Verb phrase\n",
    "AP: {<A.*>*}            # Adjective phrase\n",
    "\"\"\"\n",
    "# 정규표현식 파서\n",
    "parser = nltk.RegexpParser(grammar)\n",
    "chunks = parser.parse(words)\n",
    "\n",
    "# 결과 출력\n",
    "print(chunks)\n",
    "\n",
    "# 결과를 트리로 표시\n",
    "chunks.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
